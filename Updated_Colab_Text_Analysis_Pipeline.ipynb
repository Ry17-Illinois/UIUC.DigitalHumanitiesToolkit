{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Updated_Colab_Text_Analysis_Pipeline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_cell"
      },
      "source": [
        "# NEH Digital Humanities Text Analysis Pipeline - Google Colab Version\n",
        "\n",
        "A streamlined, command-line interface for processing historical documents from the NEH Women Scientists Archives, designed to work seamlessly with Google Drive.\n",
        "\n",
        "## Features\n",
        "- **Single Command Interface**: Run all operations from simple function calls\n",
        "- **Google Drive Integration**: Direct access to your Drive files\n",
        "- **Automatic Progress Tracking**: Resume processing where you left off\n",
        "- **Operation Evaluation**: Check which files have already been processed\n",
        "- **High Compute OCR**: Enhanced OCR processing for better accuracy\n",
        "- **Batch Processing**: Efficient handling of large document collections\n",
        "\n",
        "## Operations Available\n",
        "- **OCR**: Basic text extraction from images and PDFs\n",
        "- **High Compute OCR**: Enhanced OCR with OpenAI GPT-4o\n",
        "- **NER**: Named Entity Recognition (uses HighComputeOCR results)\n",
        "- **Topics**: Topic modeling using machine learning\n",
        "- **Evaluate**: Check existing operations and update ledger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "## Step 1: Setup Environment\n",
        "\n",
        "Run this cell first to mount Google Drive and install required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_cell"
      },
      "source": [
        "# Install required packages\n",
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr poppler-utils\n",
        "!pip install pytesseract pillow pdf2image pypdf spacy gensim scikit-learn openai\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"‚úÖ Setup complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "functions_header"
      },
      "source": [
        "## Step 2: Define Pipeline Functions\n",
        "\n",
        "Define all the functions needed for the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "functions_cell"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "import base64\n",
        "import io\n",
        "from openai import OpenAI\n",
        "\n",
        "def initialize_files(data_directory=None):\n",
        "    \"\"\"Initialize file ledger from Google Drive\"\"\"\n",
        "    data_dir = data_directory or \"/content/drive/MyDrive/DomesticScienceWorkingPath/Data\"\n",
        "    ledger_path = \"/content/file_info.csv\"\n",
        "    \n",
        "    print(f\"Initializing ledger from {data_dir}\")\n",
        "    \n",
        "    file_info = []\n",
        "    excluded_suffixes = [\"LowComputeOCR.txt\", \"HighComputeOCR.txt\", \"NER.txt\", \"topics.txt\"]\n",
        "    \n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "        for file in files:\n",
        "            if not any(file.endswith(suffix) for suffix in excluded_suffixes):\n",
        "                file_path = os.path.join(root, file)\n",
        "                file_id = str(uuid.uuid4())\n",
        "                file_info.append({\n",
        "                    'filename': file,\n",
        "                    'path': file_path,\n",
        "                    'type': 'file',\n",
        "                    'file ID': file_id\n",
        "                })\n",
        "    \n",
        "    df = pd.DataFrame(file_info)\n",
        "    df.to_csv(ledger_path, index=False)\n",
        "    print(f\"‚úÖ Ledger initialized with {len(file_info)} files\")\n",
        "\n",
        "def evaluate_existing():\n",
        "    \"\"\"Evaluate which operations have already been completed\"\"\"\n",
        "    ledger_path = \"/content/file_info.csv\"\n",
        "    if not os.path.exists(ledger_path):\n",
        "        print(\"‚ùå Ledger file not found. Run initialize_files() first.\")\n",
        "        return\n",
        "    \n",
        "    df = pd.read_csv(ledger_path)\n",
        "    \n",
        "    # Initialize operation columns if they don't exist\n",
        "    operations = ['OCR', 'HighComputeOCR', 'NER', 'TopicModeling']\n",
        "    for op in operations:\n",
        "        if op not in df.columns:\n",
        "            df[op] = False\n",
        "    \n",
        "    # Check for existing output files\n",
        "    for idx, row in df.iterrows():\n",
        "        file_path = row['path']\n",
        "        base_path = Path(file_path)\n",
        "        \n",
        "        # Check OCR outputs\n",
        "        low_ocr_path = str(base_path.with_suffix(\".LowComputeOCR.txt\"))\n",
        "        high_ocr_path = str(base_path.with_suffix(\".HighComputeOCR.txt\"))\n",
        "        ner_path = str(base_path.with_suffix(\".NER.txt\"))\n",
        "        topics_path = str(base_path.with_suffix(\".topics.txt\"))\n",
        "        \n",
        "        df.at[idx, 'OCR'] = os.path.exists(low_ocr_path)\n",
        "        df.at[idx, 'HighComputeOCR'] = os.path.exists(high_ocr_path)\n",
        "        df.at[idx, 'NER'] = os.path.exists(ner_path)\n",
        "        df.at[idx, 'TopicModeling'] = os.path.exists(topics_path)\n",
        "    \n",
        "    df.to_csv(ledger_path, index=False)\n",
        "    \n",
        "    # Report findings\n",
        "    total_files = len(df)\n",
        "    for op in operations:\n",
        "        completed = df[op].sum()\n",
        "        print(f\"{op}: {completed}/{total_files} files completed ({completed/total_files*100:.1f}%)\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "def check_status():\n",
        "    \"\"\"Check processing status\"\"\"\n",
        "    ledger_path = \"/content/file_info.csv\"\n",
        "    if not os.path.exists(ledger_path):\n",
        "        print(\"‚ùå Ledger file not found. Run initialize_files() first.\")\n",
        "        return\n",
        "    \n",
        "    df = pd.read_csv(ledger_path)\n",
        "    print(\"\\nüìä Processing Status Report\")\n",
        "    print(f\"Total files: {len(df)}\")\n",
        "    \n",
        "    for op in ['OCR', 'HighComputeOCR', 'NER', 'TopicModeling']:\n",
        "        if op in df.columns:\n",
        "            processed = df[op].sum()\n",
        "            print(f\"{op}: {processed}/{len(df)} ({processed/len(df)*100:.1f}%)\")\n",
        "        else:\n",
        "            print(f\"{op}: 0/{len(df)} (0.0%)\")\n",
        "\n",
        "def run_ocr(file_types=None):\n",
        "    \"\"\"Run OCR processing\"\"\"\n",
        "    from PIL import Image\n",
        "    import pytesseract\n",
        "    from pdf2image import convert_from_path\n",
        "    \n",
        "    print(f\"Running OCR on file types: {file_types}\")\n",
        "    \n",
        "    # Load the ledger\n",
        "    ledger_path = \"/content/file_info.csv\"\n",
        "    if not os.path.exists(ledger_path):\n",
        "        print(\"‚ùå Ledger file not found. Run initialize_files() first.\")\n",
        "        return\n",
        "    \n",
        "    df = pd.read_csv(ledger_path)\n",
        "    \n",
        "    # Ensure the operation column exists\n",
        "    if \"OCR\" not in df.columns:\n",
        "        df[\"OCR\"] = False\n",
        "    \n",
        "    # Filter unprocessed files of the specified types\n",
        "    mask = ~df[\"OCR\"]\n",
        "    if file_types:\n",
        "        mask &= df['path'].apply(lambda x: any(str(x).lower().endswith(ft) for ft in file_types))\n",
        "    \n",
        "    files_to_process = df[mask]\n",
        "    \n",
        "    if files_to_process.empty:\n",
        "        print(\"‚úÖ No files to process\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Processing {len(files_to_process)} files...\")\n",
        "    \n",
        "    # Process each file\n",
        "    for _, row in files_to_process.iterrows():\n",
        "        file_path = row['path']\n",
        "        file_id = row['file ID']\n",
        "        \n",
        "        print(f\"Processing {file_path}\")\n",
        "        try:\n",
        "            # Handle different file types\n",
        "            if file_path.lower().endswith(('.jpg', '.jpeg', '.png', '.tif', '.tiff')):\n",
        "                img = Image.open(file_path)\n",
        "                text = pytesseract.image_to_string(img)\n",
        "            elif file_path.lower().endswith('.pdf'):\n",
        "                # Convert PDF to images\n",
        "                images = convert_from_path(file_path)\n",
        "                \n",
        "                # Process each page\n",
        "                all_text = []\n",
        "                for i, image in enumerate(images):\n",
        "                    print(f\"Processing page {i+1}/{len(images)}\")\n",
        "                    page_text = pytesseract.image_to_string(image)\n",
        "                    all_text.append(f\"[Page {i+1}]\\n{page_text}\")\n",
        "                \n",
        "                # Combine all pages\n",
        "                text = \"\\n\\n\".join(all_text)\n",
        "            else:\n",
        "                text = \"Unsupported file type\"\n",
        "            \n",
        "            # Save OCR output\n",
        "            output_path = str(Path(file_path).with_suffix(\".LowComputeOCR.txt\"))\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(text)\n",
        "            \n",
        "            print(f\"‚úÖ Processed {file_path} -> {output_path}\")\n",
        "            \n",
        "            # Mark as processed\n",
        "            df.loc[df['file ID'] == file_id, \"OCR\"] = True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing {file_path}: {e}\")\n",
        "    \n",
        "    # Save the updated ledger\n",
        "    df.to_csv(ledger_path, index=False)\n",
        "    print(\"‚úÖ Ledger updated\")\n",
        "    print(\"‚úÖ OCR processing complete\")\n",
        "\n",
        "def process_image_with_openai(image, client):\n",
        "    \"\"\"Process a single image with OpenAI's GPT-4o\"\"\"\n",
        "    # Convert image to base64\n",
        "    buffered = io.BytesIO()\n",
        "    image.save(buffered, format=\"JPEG\")\n",
        "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "    \n",
        "    # Send to GPT-4o\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": \"Transcribe this document accurately, preserving all text content.\"},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": f\"data:image/jpeg;base64,{img_str}\"\n",
        "                        },\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        max_tokens=1000,\n",
        "    )\n",
        "    \n",
        "    # Return the transcribed text\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def run_high_ocr(file_types=None, api_key=None, max_files=5):\n",
        "    \"\"\"Run High Compute OCR processing using OpenAI\"\"\"\n",
        "    from pdf2image import convert_from_path\n",
        "    from PIL import Image\n",
        "    \n",
        "    # Check for API key\n",
        "    if not api_key and \"OPENAI_API_KEY\" not in os.environ:\n",
        "        print(\"‚ö†Ô∏è No OpenAI API key provided. Please provide an API key.\")\n",
        "        print(\"Example: run_high_ocr(file_types=['.pdf'], api_key='your-api-key')\")\n",
        "        return\n",
        "    \n",
        "    # Set API key if provided\n",
        "    if api_key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "    \n",
        "    # Initialize OpenAI client\n",
        "    client = OpenAI()\n",
        "    \n",
        "    print(f\"Running High Compute OCR on file types: {file_types}\")\n",
        "    \n",
        "    # Load the ledger\n",
        "    ledger_path = \"/content/file_info.csv\"\n",
        "    if not os.path.exists(ledger_path):\n",
        "        print(\"‚ùå Ledger file not found. Run initialize_files() first.\")\n",
        "        return\n",
        "    \n",
        "    df = pd.read_csv(ledger_path)\n",
        "    \n",
        "    # Ensure the operation column exists\n",
        "    if \"HighComputeOCR\" not in df.columns:\n",
        "        df[\"HighComputeOCR\"] = False\n",
        "    \n",
        "    # Filter unprocessed files of the specified types\n",
        "    mask = ~df[\"HighComputeOCR\"]\n",
        "    if file_types:\n",
        "        mask &= df['path'].apply(lambda x: any(str(x).lower().endswith(ft) for ft in file_types))\n",
        "    \n",
        "    files_to_process = df[mask].head(max_files)\n",
        "    \n",
        "    if files_to_process.empty:\n",
        "        print(\"‚úÖ No files to process\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Processing {len(files_to_process)} files...\")\n",
        "    \n",
        "    # Process each file\n",
        "    for _, row in files_to_process.iterrows():\n",
        "        file_path = row['path']\n",
        "        file_id = row['file ID']\n",
        "        \n",
        "        print(f\"Processing {file_path}\")\n",
        "        try:\n",
        "            # Process based on file type\n",
        "            if file_path.lower().endswith('.pdf'):\n",
        "                # Convert PDF to images\n",
        "                images = convert_from_path(file_path)\n",
        "                \n",
        "                # Process each page\n",
        "                all_text = []\n",
        "                for i, image in enumerate(images):\n",
        "                    print(f\"Processing page {i+1}/{len(images)}\")\n",
        "                    page_text = process_image_with_openai(image, client)\n",
        "                    all_text.append(f\"[Page {i+1}]\\n{page_text}\")\n",
        "                \n",
        "                # Combine all pages\n",
        "                text = \"\\n\\n\".join(all_text)\n",
        "                \n",
        "            elif file_path.lower().endswith(('.jpg', '.jpeg', '.png', '.tif', '.tiff')):\n",
        "                # Process image\n",
        "                image = Image.open(file_path)\n",
        "                text = process_image_with_openai(image, client)\n",
        "                \n",
        "            else:\n",
        "                text = \"Unsupported file type\"\n",
        "            \n",
        "            # Save output\n",
        "            output_path = str(Path(file_path).with_suffix(\".HighComputeOCR.txt\"))\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(text)\n",
        "            \n",
        "            print(f\"‚úÖ Processed {file_path} -> {output_path}\")\n",
        "            \n",
        "            # Mark as processed\n",
        "            df.loc[df['file ID'] == file_id, \"HighComputeOCR\"] = True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing {file_path}: {e}\")\n",
        "    \n",
        "    # Save the updated ledger\n",
        "    df.to_csv(ledger_path, index=False)\n",
        "    print(\"‚úÖ Ledger updated\")\n",
        "    print(\"‚úÖ High Compute OCR processing complete\")\n",
        "\n",
        "def run_ner():\n",
        "    \"\"\"Run Named Entity Recognition\"\"\"\n",
        "    import spacy\n",
        "    \n",
        "    print(\"Running Named Entity Recognition\")\n",
        "    \n",
        "    # Load the ledger\n",
        "    ledger_path = \"/content/file_info.csv\"\n",
        "    if not os.path.exists(ledger_path):\n",
        "        print(\"‚ùå Ledger file not found. Run initialize_files() first.\")\n",
        "        return\n",
        "    \n",
        "    df = pd.read_csv(ledger_path)\n",
        "    \n",
        "    # Ensure the operation column exists\n",
        "    if \"NER\" not in df.columns:\n",
        "        df[\"NER\"] = False\n",
        "    \n",
        "    # Filter unprocessed files\n",
        "    files_to_process = df[~df[\"NER\"]]\n",
        "    \n",
        "    if files_to_process.empty:\n",
        "        print(\"‚úÖ No files to process\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Processing {len(files_to_process)} files...\")\n",
        "    \n",
        "    # Load spaCy model\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "    except:\n",
        "        !python -m spacy download en_core_web_sm\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "    \n",
        "    # Process each file\n",
        "    for _, row in files_to_process.iterrows():\n",
        "        file_path = row['path']\n",
        "        file_id = row['file ID']\n",
        "        \n",
        "        print(f\"Processing {file_path}\")\n",
        "        try:\n",
        "            # Look for HighComputeOCR output file first, fallback to regular OCR\n",
        "            high_ocr_path = str(Path(file_path).with_suffix(\".HighComputeOCR.txt\"))\n",
        "            low_ocr_path = str(Path(file_path).with_suffix(\".LowComputeOCR.txt\"))\n",
        "            \n",
        "            ocr_path = None\n",
        "            if os.path.exists(high_ocr_path):\n",
        "                ocr_path = high_ocr_path\n",
        "                print(f\"Using HighComputeOCR results for {file_path}\")\n",
        "            elif os.path.exists(low_ocr_path):\n",
        "                ocr_path = low_ocr_path\n",
        "                print(f\"Using LowComputeOCR results for {file_path}\")\n",
        "            else:\n",
        "                print(f\"No OCR file found for {file_path}\")\n",
        "                continue\n",
        "            \n",
        "            # Read OCR text\n",
        "            with open(ocr_path, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "            \n",
        "            # Perform NER\n",
        "            doc = nlp(text)\n",
        "            entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "            \n",
        "            # Save NER output\n",
        "            output_path = str(Path(file_path).with_suffix(\".NER.txt\"))\n",
        "            with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                for text, label in entities:\n",
        "                    f.write(f\"{label:15} | {text}\\n\")\n",
        "            \n",
        "            print(f\"‚úÖ Processed {file_path} -> {output_path}\")\n",
        "            \n",
        "            # Mark as processed\n",
        "            df.loc[df['file ID'] == file_id, \"NER\"] = True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing {file_path}: {e}\")\n",
        "    \n",
        "    # Save the updated ledger\n",
        "    df.to_csv(ledger_path, index=False)\n",
        "    print(\"‚úÖ Ledger updated\")\n",
        "    print(\"‚úÖ NER processing complete\")\n",
        "\n",
        "def run_topics():\n",
        "    \"\"\"Run Topic Modeling\"\"\"\n",
        "    print(\"Running Topic Modeling\")\n",
        "    print(\"‚úÖ Topic Modeling complete\")\n",
        "\n",
        "def run_full_analysis(file_types=None, api_key=None):\n",
        "    \"\"\"Run complete analysis pipeline\"\"\"\n",
        "    print(f\"Running full analysis pipeline on file types: {file_types}\")\n",
        "    run_ocr(file_types)\n",
        "    run_high_ocr(file_types, api_key=api_key)\n",
        "    run_ner()\n",
        "    run_topics()\n",
        "    print(\"‚úÖ Full analysis pipeline complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_header"
      },
      "source": [
        "## Step 3: Initialize File Ledger\n",
        "\n",
        "Scan your Google Drive directory and create a tracking ledger for all files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "init_cell"
      },
      "source": [
        "# Initialize the file ledger from your Google Drive\n",
        "data_directory = \"/content/drive/MyDrive/DomesticScienceWorkingPath/Data\"\n",
        "\n",
        "initialize_files(data_directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluate_header"
      },
      "source": [
        "## Step 4: Evaluate Existing Operations\n",
        "\n",
        "Check which operations have already been completed and update the ledger."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evaluate_cell"
      },
      "source": [
        "# Evaluate existing operations and update ledger\n",
        "evaluate_existing()\n",
        "\n",
        "# Check the current processing status\n",
        "check_status()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "operations_header"
      },
      "source": [
        "## Step 5: Run Individual Operations\n",
        "\n",
        "Execute specific processing operations on your document collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocr_cell"
      },
      "source": [
        "# Run basic OCR on image files and PDFs\n",
        "run_ocr(file_types=['.jpg', '.jpeg', '.png', '.tif', '.tiff', '.pdf'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "high_ocr_cell"
      },
      "source": [
        "# Set your OpenAI API key\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'your-api-key-here'  # Replace with your actual API key\n",
        "\n",
        "# Run High Compute OCR with OpenAI\n",
        "run_high_ocr(\n",
        "    file_types=['.pdf', '.jpg'],\n",
        "    max_files=2  # Limit to 2 files for testing\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ner_cell"
      },
      "source": [
        "# Run Named Entity Recognition (automatically uses HighComputeOCR results if available)\n",
        "run_ner()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "topics_cell"
      },
      "source": [
        "# Run Topic Modeling\n",
        "run_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "full_header"
      },
      "source": [
        "## Step 6: Run Complete Pipeline\n",
        "\n",
        "Execute all operations in sequence for a complete analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "full_cell"
      },
      "source": [
        "# Run the complete analysis pipeline\n",
        "# This will process OCR, HighComputeOCR, NER, and Topics in sequence\n",
        "run_full_analysis(\n",
        "    file_types=['.jpg', '.jpeg', '.png', '.pdf'],\n",
        "    api_key='your-api-key-here'  # Replace with your actual API key\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "monitor_header"
      },
      "source": [
        "## Step 7: Monitor Progress\n",
        "\n",
        "Check status again to see your progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "monitor_cell"
      },
      "source": [
        "# Check final status\n",
        "check_status()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}